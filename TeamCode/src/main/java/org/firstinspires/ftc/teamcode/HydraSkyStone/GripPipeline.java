//package org.firstinspires.ftc.teamcode.HydraSkyStone;
//
//import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
//import com.qualcomm.robotcore.util.ElapsedTime;
//
//import java.io.File;
//import java.io.FileWriter;
//import java.io.IOException;
//import java.util.ArrayList;
//import java.util.List;
//import java.util.Map;
//import java.util.stream.Collectors;
//import java.util.HashMap;
//
//import org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;
//import org.opencv.core.*;
//import org.opencv.core.Core.*;
////import org.opencv.features2d.FeatureDetector;
//import org.opencv.imgcodecs.Imgcodecs;
//import org.opencv.imgproc.*;
//import org.opencv.objdetect.*;
//import org.openftc.easyopencv.OpenCvCamera;
//import org.openftc.easyopencv.OpenCvCameraRotation;
//import org.openftc.easyopencv.OpenCvPipeline;
//import org.openftc.easyopencv.OpenCvWebcam;
//
///**
//* GripPipeline class.
//*
//* <p>An OpenCV pipeline generated by GRIP.
//*
//* @author GRIP
//*/
//public class GripPipeline extends LinearOpMode{
//
//	OpenCvCamera webcam;
//	LinearOpMode opMode;
//
//	public GripPipeline(LinearOpMode opMode) throws InterruptedException {
//		this.opMode = opMode;
//		int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier("cameraMonitorViewId", "id", hardwareMap.appContext.getPackageName());
//		webcam = new OpenCvWebcam(hardwareMap.get(WebcamName.class, "Webcam 1"), cameraMonitorViewId);
//		ElapsedTime times;
//	}
//
//	public void runOpMode() throws InterruptedException {
//		/*
//		 * Instantiate an OpenCvCamera object for the camera we'll be using.
//		 * In this sample, we're using a webcam. Note that you will need to
//		 * make sure you have added the webcam to your configuration file and
//		 * adjusted the name here to match what you named it in said config file.
//		 *
//		 * We pass it the view that we wish to use for camera monitor (on
//		 * the RC phone). If no camera monitor is desired, use the alternate
//		 * single-parameter constructor instead (commented out below)
//		 */
//
//		// OR...  Do Not Activate the Camera Monitor View
//		//webcam = new OpenCvWebcam(hardwareMap.get(WebcamName.class, "Webcam 1"));
//
//		/*
//		 * Open the connection to the camera device
//		 */
//		webcam.openCameraDevice();
//
//		/*
//		 * Specify the image processing pipeline we wish to invoke upon receipt
//		 * of a frame from the camera. Note that switching pipelines on-the-fly
//		 * (while a streaming session is in flight) *IS* supported.
//		 */
//		webcam.setPipeline(new GripPipeline.SamplePipeline());
//
//		/*
//		 * Tell the webcam to start streaming images to us! Note that you must make sure
//		 * the resolution you specify is supported by the camera. If it is not, an exception
//		 * will be thrown.
//		 *
//		 * Keep in mind that the SDK's UVC driver (what OpenCvWebcam uses under the hood) only
//		 * supports streaming from the webcam in the uncompressed YUV image format. This means
//		 * that the maximum resolution you can stream uut and still get up to 30FPS is 480p (640x480).
//		 * Streaming at 720p will limit you to up to 10FPS. However, streaming at frame rates other
//		 * than 30FPS is not currently supported, although this will likely be addressed in a future
//		 * release. TLDR: You can't stream in greater than 480p from a webcam at the moment.
//		 *
//		 * Also, we specify the rotation that the webcam is used in. This is so that the image
//		 * from the camera sensor can be rotated such that it is always displayed with the image upright.
//		 * For a front facing camera, rotation is defined assuming the user is looking at the screen.
//		 * For a urear facing camera or a webcam, rotation is defined assuming the camera is facing
//		 * away from the user.
//		 */
//
//		webcam.startStreaming(320, 240, OpenCvCameraRotation.UPRIGHT);
//
//		/*
//		 * Wait for the user to press start on the Driver Station
//		 */
//		waitForStart();
//
//		while (opModeIsActive()) {
//			/*
//			 * Send some stats to the telemetry
//			 */
//			telemetry.addData("Frame Count", webcam.getFrameCount());
//			telemetry.addData("FPS", String.format("%.2f", webcam.getFps()));
//			telemetry.addData("Total frame time ms", webcam.getTotalFrameTimeMs());
//			telemetry.addData("Pipeline time ms", webcam.getPipelineTimeMs());
//			telemetry.addData("Overhead time ms", webcam.getOverheadTimeMs());
//			telemetry.addData("Theoretical max FPS", webcam.getCurrentPipelineMaxFps());
//			telemetry.update();
//
//			/*
//			 * NOTE: stopping the stream from the camera early (before the end of the OpMode
//			 * when it will be automatically stopped for you) *IS* supported. The "if" statement
//			 * below will stop streaming from the camera when the "A" button on gamepad 1 is pressed.
//			 */
////            if(gamepad2.a) {
////                /*
////                 * IMPORTANT NOTE: calling stopStreaming() will indeed stop the stream of images
////                 * from the camera (and, by extension, stop calling your vision pipeline). HOWEVER,
////                 * if the reason you wish to stop the stream early is to switch use of the camera
////                 * over to, say, Vuforia or TFOD, you will also need to call closeCameraDevice()
////                 * (commented out below), because according to the Android Camera API documentation:
////                 *         "Your application should only have one Camera object active at a time for
////                 *          a particular hardware camera."
////                 *
////                 * NB: calling closeCameraDevice() will internally call stopStreaming() if applicable,
////                 * but it doesn't hurt to call it anyway, if for no other reason than clarity.
////                 *
////                 * NB2: if you are stopping the camera stream to simply save some processing power
////                 * (or battery power) for a short while when you do not need your vision pipeline,
////                 * it is recommended to NOT call closeCameraDevice() as you will then need to re-open
////                 * it the next time you wish to activate your vision pipeline, which can take a bit of
////                 * time. Of course, this comment is irrelevant in light of the use case described in
////                 * the above "important note".
////                 */
////                webcam.stopStreaming();
////                //webcam.closeCameraDevice();
////            }
////
////            /*
////             * The viewport (if one was specified in the constructor) can also be dynamically "paused"
////             * and "resumed". The primary use case of this is to reduce CPU, memory, and power load
////             * when you need your vision pipeline running, but do not require a live preview on the
////             * robot controller screen. For instance, this could be useful if you wish to see the live
////             * camera preview as you are initializing your robot, but you no longer require the live
////             * preview after you have finished your initialization process; pausing the viewport does
////             * not stop running your pipeline.
////             *
////             * The "if" statements below will pause the viewport if the "X" button on gamepad1 is pressed,
////             * and resume the viewport if the "Y" button on gamepad1 is pressed.
////             */
////            else if(gamepad2.x) {
////                webcam.pauseViewport();
////            }
////            else if(gamepad2.y) {
////                webcam.resumeViewport();
////            }
//
//			/*
//			 * For the purposes of this sample, throttle ourselves to 10Hz loop to avoid burning
//			 * excess CPU cycles for no reason. (By default, telemetry is only sent to the DS at 4Hz
//			 * anyway). Of course in a real OpMode you will likely not want to do this.
//			 */
//			sleep(100);
//		}
//	}
//
//	class SamplePipeline extends OpenCvPipeline{
//
//		//Outputs
//		private Mat cvErodeOutput = new Mat();
//		private Mat rgbThresholdOutput = new Mat();
//		private MatOfKeyPoint findBlobsOutput = new MatOfKeyPoint();
//
//		/**
//		 * This is the primary method that runs the entire pipeline and updates the outputs.
//		 */
//		@Override
//		public Mat processFrame(Mat source0) {
//			// Step CV_erode0:
//			Mat cvErodeSrc = source0;
//			Mat cvErodeKernel = new Mat();
//			Point cvErodeAnchor = new Point(-1, -1);
//			double cvErodeIterations = 20.0;
//			int cvErodeBordertype = Core.BORDER_CONSTANT;
//			Scalar cvErodeBordervalue = new Scalar(-1);
//			cvErode(cvErodeSrc, cvErodeKernel, cvErodeAnchor, cvErodeIterations, cvErodeBordertype, cvErodeBordervalue, cvErodeOutput);
//
//			// Step RGB_Threshold0:
//			Mat rgbThresholdInput = cvErodeOutput;
//			double[] rgbThresholdRed = {0.0, 22.192832764505116};
//			double[] rgbThresholdGreen = {0.0, 30.895904436860075};
//			double[] rgbThresholdBlue = {0.0, 30.895904436860064};
//			rgbThreshold(rgbThresholdInput, rgbThresholdRed, rgbThresholdGreen, rgbThresholdBlue, rgbThresholdOutput);
//
//			// Step Find_Blobs0:
//			Mat findBlobsInput = rgbThresholdOutput;
//			double findBlobsMinArea = 10.0;
//			double[] findBlobsCircularity = {0.0, 1.0};
//			boolean findBlobsDarkBlobs = false;
//			findBlobs(findBlobsInput, findBlobsMinArea, findBlobsCircularity, findBlobsDarkBlobs, findBlobsOutput);
//
//			return source0;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a CV_erode.
//		 * @return Mat output from CV_erode.
//		 */
//		public Mat cvErodeOutput() {
//			return cvErodeOutput;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a RGB_Threshold.
//		 * @return Mat output from RGB_Threshold.
//		 */
//		public Mat rgbThresholdOutput() {
//			return rgbThresholdOutput;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a Find_Blobs.
//		 * @return MatOfKeyPoint output from Find_Blobs.
//		 */
//		public MatOfKeyPoint findBlobsOutput() {
//			return findBlobsOutput;
//		}
//
//
//		/**
//		 * Expands area of lower value in an image.
//		 * @param src the Image to erode.
//		 * @param kernel the kernel for erosion.
//		 * @param anchor the center of the kernel.
//		 * @param iterations the number of times to perform the erosion.
//		 * @param borderType pixel extrapolation method.
//		 * @param borderValue value to be used for a constant border.
//		 * @param dst Output Image.
//		 */
//		private void cvErode(Mat src, Mat kernel, Point anchor, double iterations,
//							 int borderType, Scalar borderValue, Mat dst) {
//			if (kernel == null) {
//				kernel = new Mat();
//			}
//			if (anchor == null) {
//				anchor = new Point(-1,-1);
//			}
//			if (borderValue == null) {
//				borderValue = new Scalar(-1);
//			}
//			Imgproc.erode(src, dst, kernel, anchor, (int)iterations, borderType, borderValue);
//		}
//
//		/**
//		 * Segment an image based on color ranges.
//		 * @param input The image on which to perform the RGB threshold.
//		 * @param red The min and max red.
//		 * @param green The min and max green.
//		 * @param blue The min and max blue.
//		//* @param output The image in which to store the output.
//		 */
//		private void rgbThreshold(Mat input, double[] red, double[] green, double[] blue,
//								  Mat out) {
//			Imgproc.cvtColor(input, out, Imgproc.COLOR_BGR2RGB);
//			Core.inRange(out, new Scalar(red[0], green[0], blue[0]),
//					new Scalar(red[1], green[1], blue[1]), out);
//		}
//
//		/**
//		 * Detects groups of pixels in an image.
//		 * @param input The image on which to perform the find blobs.
//		 * @param minArea The minimum size of a blob that will be found
//		 * @param circularity The minimum and maximum circularity of blobs that will be found
//		 * @param darkBlobs The boolean that determines if light or dark blobs are found.
//		 * @param blobList The output where the MatOfKeyPoint is stored.
//		 */
//		private void findBlobs(Mat input, double minArea, double[] circularity,
//							   Boolean darkBlobs, MatOfKeyPoint blobList) {
//			//FeatureDetector blobDet = FeatureDetector.create(FeatureDetector.SIMPLEBLOB);
//			try {
//				File tempFile = File.createTempFile("config", ".xml");
//
//				StringBuilder config = new StringBuilder();
//
//				config.append("<?xml version=\"1.0\"?>\n");
//				config.append("<opencv_storage>\n");
//				config.append("<thresholdStep>10.</thresholdStep>\n");
//				config.append("<minThreshold>50.</minThreshold>\n");
//				config.append("<maxThreshold>220.</maxThreshold>\n");
//				config.append("<minRepeatability>2</minRepeatability>\n");
//				config.append("<minDistBetweenBlobs>10.</minDistBetweenBlobs>\n");
//				config.append("<filterByColor>1</filterByColor>\n");
//				config.append("<blobColor>");
//				config.append((darkBlobs ? 0 : 255));
//				config.append("</blobColor>\n");
//				config.append("<filterByArea>1</filterByArea>\n");
//				config.append("<minArea>");
//				config.append(minArea);
//				config.append("</minArea>\n");
//				config.append("<maxArea>");
//				config.append(Integer.MAX_VALUE);
//				config.append("</maxArea>\n");
//				config.append("<filterByCircularity>1</filterByCircularity>\n");
//				config.append("<minCircularity>");
//				config.append(circularity[0]);
//				config.append("</minCircularity>\n");
//				config.append("<maxCircularity>");
//				config.append(circularity[1]);
//				config.append("</maxCircularity>\n");
//				config.append("<filterByInertia>1</filterByInertia>\n");
//				config.append("<minInertiaRatio>0.1</minInertiaRatio>\n");
//				config.append("<maxInertiaRatio>" + Integer.MAX_VALUE + "</maxInertiaRatio>\n");
//				config.append("<filterByConvexity>1</filterByConvexity>\n");
//				config.append("<minConvexity>0.95</minConvexity>\n");
//				config.append("<maxConvexity>" + Integer.MAX_VALUE + "</maxConvexity>\n");
//				config.append("</opencv_storage>\n");
//				FileWriter writer;
//				writer = new FileWriter(tempFile, false);
//				writer.write(config.toString());
//				writer.close();
//				//blobDet.read(tempFile.getPath());
//			} catch (IOException e) {
//				e.printStackTrace();
//			}
//
//			//blobDet.detect(input, blobList);
//		}
//
//	}
//}
//
