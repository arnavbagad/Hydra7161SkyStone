//package org.firstinspires.ftc.teamcode;
//
//import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
//import com.qualcomm.robotcore.util.ElapsedTime;
//
//import java.io.File;
//import java.io.FileWriter;
//import java.io.IOException;
//import java.util.ArrayList;
//import java.util.List;
//import java.util.Map;
//import java.util.stream.Collectors;
//import java.util.HashMap;
//
//import org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;
//import org.opencv.core.*;
//import org.opencv.core.Core.*;
////import org.opencv.features2d.FeatureDetector;
//import org.opencv.imgcodecs.Imgcodecs;
//import org.opencv.imgproc.*;
//import org.opencv.objdetect.*;
//import org.openftc.easyopencv.OpenCvCamera;
//import org.openftc.easyopencv.OpenCvCameraRotation;
//import org.openftc.easyopencv.OpenCvPipeline;
//import org.openftc.easyopencv.OpenCvWebcam;
//
///**
//* GripPipelineGOD class.
//*
//* <p>An OpenCV pipeline generated by GRIP.
//*
//* @author GRIP
//*/
//
//public class GripPipelineGOD{
//
//	OpenCvCamera webcam;
//	SamplePipeline samplePipeline;
//	LinearOpMode opMode;
//
//	public GripPipelineGOD(LinearOpMode opMode) throws InterruptedException {
//		this.opMode = opMode;
//		int cameraMonitorViewId = opMode.hardwareMap.appContext.getResources().getIdentifier("cameraMonitorViewId", "id", opMode.hardwareMap.appContext.getPackageName());
//		webcam = new OpenCvWebcam(opMode.hardwareMap.get(WebcamName.class, "Webcam 1"), cameraMonitorViewId);
//		ElapsedTime times;
////		webcam.openCameraDevice();
////		webcam.setPipeline(new GripPipelineGOD.SamplePipeline());
////		webcam.startStreaming(320, 240, OpenCvCameraRotation.UPRIGHT);
////		webcam.pauseViewport();
//	}
//
//	public String pipeLine() throws InterruptedException {
//		webcam.openCameraDevice();
//		webcam.setPipeline(new GripPipelineGOD.SamplePipeline());
//		webcam.startStreaming(320, 240, OpenCvCameraRotation.UPRIGHT);
//		webcam.pauseViewport();
//		return null;
//	}
//
//	public void runOpMode() throws InterruptedException {
//		/*
//		 * Instantiate an OpenCvCamera object for the camera we'll be using.
//		 * In this sample, we're using a webcam. Note that you will need to
//		 * make sure you have added the webcam to your configuration file and
//		 * adjusted the name here to match what you named it in said config file.
//		 *
//		 * We pass it the view that we wish to use for camera monitor (on
//		 * the RC phone). If no camera monitor is desired, use the alternate
//		 * single-parameter constructor instead (commented out below)
//		 */
//
//		// OR...  Do Not Activate the Camera Monitor View
//		//webcam = new OpenCvWebcam(hardwareMap.get(WebcamName.class, "Webcam 1"));
//
//		/*
//		 * Open the connection to the camera device
//		 */
//		webcam.openCameraDevice();
//
//		/*
//		 * Specify the image processing pipeline we wish to invoke upon receipt
//		 * of a frame from the camera. Note that switching pipelines on-the-fly
//		 * (while a streaming session is in flight) *IS* supported.
//		 */
//		webcam.setPipeline(new GripPipelineGOD.SamplePipeline());
//
//		/*
//		 * Tell the webcam to start streaming images to us! Note that you must make sure
//		 * the resolution you specify is supported by the camera. If it is not, an exception
//		 * will be thrown.
//		 *
//		 * Keep in mind that the SDK's UVC driver (what OpenCvWebcam uses under the hood) only
//		 * supports streaming from the webcam in the uncompressed YUV image format. This means
//		 * that the maximum resolution you can stream uut and still get up to 30FPS is 480p (640x480).
//		 * Streaming at 720p will limit you to up to 10FPS. However, streaming at frame rates other
//		 * than 30FPS is not currently supported, although this will likely be addressed in a future
//		 * release. TLDR: You can't stream in greater than 480p from a webcam at the moment.
//		 *
//		 * Also, we specify the rotation that the webcam is used in. This is so that the image
//		 * from the camera sensor can be rotated such that it is always displayed with the image upright.
//		 * For a front facing camera, rotation is defined assuming the user is looking at the screen.
//		 * For a urear facing camera or a webcam, rotation is defined assuming the camera is facing
//		 * away from the user.
//		 */
//
//		webcam.startStreaming(320, 240, OpenCvCameraRotation.UPRIGHT);
//
//		/*
//		 * Wait for the user to press start on the Driver Station
//		 */
//		opMode.waitForStart();
//
//		while (opMode.opModeIsActive()) {
//			/*
//			 * Send some stats to the telemetry
//			 */
//			opMode.telemetry.addData("Frame Count", webcam.getFrameCount());
//			opMode.telemetry.addData("FPS", String.format("%.2f", webcam.getFps()));
//			opMode.telemetry.addData("Total frame time ms", webcam.getTotalFrameTimeMs());
//			opMode.telemetry.addData("Pipeline time ms", webcam.getPipelineTimeMs());
//			opMode.telemetry.addData("Overhead time ms", webcam.getOverheadTimeMs());
//			opMode.telemetry.addData("Theoretical max FPS", webcam.getCurrentPipelineMaxFps());
//			opMode.telemetry.update();
//
//			/*
//			 * NOTE: stopping the stream from the camera early (before the end of the OpMode
//			 * when it will be automatically stopped for you) *IS* supported. The "if" statement
//			 * below will stop streaming from the camera when the "A" button on gamepad 1 is pressed.
//			 */
////            if(gamepad2.a) {
////                /*
////                 * IMPORTANT NOTE: calling stopStreaming() will indeed stop the stream of images
////                 * from the camera (and, by extension, stop calling your vision pipeline). HOWEVER,
////                 * if the reason you wish to stop the stream early is to switch use of the camera
////                 * over to, say, Vuforia or TFOD, you will also need to call closeCameraDevice()
////                 * (commented out below), because according to the Android Camera API documentation:
////                 *         "Your application should only have one Camera object active at a time for
////                 *          a particular hardware camera."
////                 *
////                 * NB: calling closeCameraDevice() will internally call stopStreaming() if applicable,
////                 * but it doesn't hurt to call it anyway, if for no other reason than clarity.
////                 *
////                 * NB2: if you are stopping the camera stream to simply save some processing power
////                 * (or battery power) for a short while when you do not need your vision pipeline,
////                 * it is recommended to NOT call closeCameraDevice() as you will then need to re-open
////                 * it the next time you wish to activate your vision pipeline, which can take a bit of
////                 * time. Of course, this comment is irrelevant in light of the use case described in
////                 * the above "important note".
////                 */
////                webcam.stopStreaming();
////                //webcam.closeCameraDevice();
////            }
////
////            /*
////             * The viewport (if one was specified in the constructor) can also be dynamically "paused"
////             * and "resumed". The primary use case of this is to reduce CPU, memory, and power load
////             * when you need your vision pipeline running, but do not require a live preview on the
////             * robot controller screen. For instance, this could be useful if you wish to see the live
////             * camera preview as you are initializing your robot, but you no longer require the live
////             * preview after you have finished your initialization process; pausing the viewport does
////             * not stop running your pipeline.
////             *
////             * The "if" statements below will pause the viewport if the "X" button on gamepad1 is pressed,
////             * and resume the viewport if the "Y" button on gamepad1 is pressed.
////             */
////            else if(gamepad2.x) {
////                webcam.pauseViewport();
////            }
////            else if(gamepad2.y) {
////                webcam.resumeViewport();
////            }
//
//			/*
//			 * For the purposes of this sample, throttle ourselves to 10Hz loop to avoid burning
//			 * excess CPU cycles for no reason. (By default, telemetry is only sent to the DS at 4Hz
//			 * anyway). Of course in a real OpMode you will likely not want to do this.
//			 */
//			opMode.sleep(100);
//		}
//	}
//
//	static class SamplePipeline extends OpenCvPipeline {
//
//		//Outputs
//		private Mat blurOutput = new Mat();
//		private Mat cvErodeOutput = new Mat();
//		private Mat hsvThresholdOutput = new Mat();
//		private MatOfKeyPoint findBlobsOutput = new MatOfKeyPoint();
//
//		static {
//			System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
//		}
//
//		/**
//		 * This is the primary method that runs the entire pipeline and updates the outputs.
//		 */
//
//		@Override
//		public Mat processFrame(Mat source0) {
//			// Step Blur0:
//			Mat blurInput = source0;
//			BlurType blurType = BlurType.get("Box Blur");
//			double blurRadius = 100.0;
//			blur(blurInput, blurType, blurRadius, blurOutput);
//
//			// Step CV_erode0:
//			Mat cvErodeSrc = blurOutput;
//			Mat cvErodeKernel = new Mat();
//			Point cvErodeAnchor = new Point(-1, -1);
//			double cvErodeIterations = 20.0;
//			int cvErodeBordertype = Core.BORDER_CONSTANT;
//			Scalar cvErodeBordervalue = new Scalar(-1);
//			cvErode(cvErodeSrc, cvErodeKernel, cvErodeAnchor, cvErodeIterations, cvErodeBordertype, cvErodeBordervalue, cvErodeOutput);
//
//			// Step HSV_Threshold0:
//			Mat hsvThresholdInput = cvErodeOutput;
//			double[] hsvThresholdHue = {0.0, 180.0};
//			double[] hsvThresholdSaturation = {0.0, 255.0};
//			double[] hsvThresholdValue = {0.0, 95.08105802047778};
//			hsvThreshold(hsvThresholdInput, hsvThresholdHue, hsvThresholdSaturation, hsvThresholdValue, hsvThresholdOutput);
//
//			// Step Find_Blobs0:
//			Mat findBlobsInput = hsvThresholdOutput;
//			double findBlobsMinArea = 2000.0;
//			double[] findBlobsCircularity = {0.0, 1.0};
//			boolean findBlobsDarkBlobs = false;
//			findBlobs(findBlobsInput, findBlobsMinArea, findBlobsCircularity, findBlobsDarkBlobs, findBlobsOutput);
//
//			return null;
//		}
//
////	public void process(Mat source0) {
////		// Step Blur0:
////		Mat blurInput = source0;
////		BlurType blurType = BlurType.get("Box Blur");
////		double blurRadius = 100.0;
////		blur(blurInput, blurType, blurRadius, blurOutput);
////
////		// Step CV_erode0:
////		Mat cvErodeSrc = blurOutput;
////		Mat cvErodeKernel = new Mat();
////		Point cvErodeAnchor = new Point(-1, -1);
////		double cvErodeIterations = 20.0;
////		int cvErodeBordertype = Core.BORDER_CONSTANT;
////		Scalar cvErodeBordervalue = new Scalar(-1);
////		cvErode(cvErodeSrc, cvErodeKernel, cvErodeAnchor, cvErodeIterations, cvErodeBordertype, cvErodeBordervalue, cvErodeOutput);
////
////		// Step HSV_Threshold0:
////		Mat hsvThresholdInput = cvErodeOutput;
////		double[] hsvThresholdHue = {0.0, 180.0};
////		double[] hsvThresholdSaturation = {0.0, 255.0};
////		double[] hsvThresholdValue = {0.0, 95.08105802047778};
////		hsvThreshold(hsvThresholdInput, hsvThresholdHue, hsvThresholdSaturation, hsvThresholdValue, hsvThresholdOutput);
////
////		// Step Find_Blobs0:
////		Mat findBlobsInput = hsvThresholdOutput;
////		double findBlobsMinArea = 2000.0;
////		double[] findBlobsCircularity = {0.0, 1.0};
////		boolean findBlobsDarkBlobs = false;
////		findBlobs(findBlobsInput, findBlobsMinArea, findBlobsCircularity, findBlobsDarkBlobs, findBlobsOutput);
////
////	}
//
//		/**
//		 * This method is a generated getter for the output of a Blur.
//		 *
//		 * @return Mat output from Blur.
//		 */
//		public Mat blurOutput() {
//			return blurOutput;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a CV_erode.
//		 *
//		 * @return Mat output from CV_erode.
//		 */
//		public Mat cvErodeOutput() {
//			return cvErodeOutput;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a HSV_Threshold.
//		 *
//		 * @return Mat output from HSV_Threshold.
//		 */
//		public Mat hsvThresholdOutput() {
//			return hsvThresholdOutput;
//		}
//
//		/**
//		 * This method is a generated getter for the output of a Find_Blobs.
//		 *
//		 * @return MatOfKeyPoint output from Find_Blobs.
//		 */
//		public MatOfKeyPoint findBlobsOutput() {
//			return findBlobsOutput;
//		}
//
//		/**
//		 * An indication of which type of filter to use for a blur.
//		 * Choices are BOX, GAUSSIAN, MEDIAN, and BILATERAL
//		 */
//		enum BlurType {
//			BOX("Box Blur"), GAUSSIAN("Gaussian Blur"), MEDIAN("Median Filter"),
//			BILATERAL("Bilateral Filter");
//
//			private final String label;
//
//			BlurType(String label) {
//				this.label = label;
//			}
//
//			public static BlurType get(String type) {
//				if (BILATERAL.label.equals(type)) {
//					return BILATERAL;
//				} else if (GAUSSIAN.label.equals(type)) {
//					return GAUSSIAN;
//				} else if (MEDIAN.label.equals(type)) {
//					return MEDIAN;
//				} else {
//					return BOX;
//				}
//			}
//
//			@Override
//			public String toString() {
//				return this.label;
//			}
//		}
//
//		/**
//		 * Softens an image using one of several filters.
//		 *
//		 * @param input        The image on which to perform the blur.
//		 * @param type         The blurType to perform.
//		 * @param doubleRadius The radius for the blur.
//		 * @param output       The image in which to store the output.
//		 */
//		private void blur(Mat input, BlurType type, double doubleRadius,
//						  Mat output) {
//			int radius = (int) (doubleRadius + 0.5);
//			int kernelSize;
//			switch (type) {
//				case BOX:
//					kernelSize = 2 * radius + 1;
//					Imgproc.blur(input, output, new Size(kernelSize, kernelSize));
//					break;
//				case GAUSSIAN:
//					kernelSize = 6 * radius + 1;
//					Imgproc.GaussianBlur(input, output, new Size(kernelSize, kernelSize), radius);
//					break;
//				case MEDIAN:
//					kernelSize = 2 * radius + 1;
//					Imgproc.medianBlur(input, output, kernelSize);
//					break;
//				case BILATERAL:
//					Imgproc.bilateralFilter(input, output, -1, radius, radius);
//					break;
//			}
//		}
//
//		/**
//		 * Expands area of lower value in an image.
//		 *
//		 * @param src         the Image to erode.
//		 * @param kernel      the kernel for erosion.
//		 * @param anchor      the center of the kernel.
//		 * @param iterations  the number of times to perform the erosion.
//		 * @param borderType  pixel extrapolation method.
//		 * @param borderValue value to be used for a constant border.
//		 * @param dst         Output Image.
//		 */
//		private void cvErode(Mat src, Mat kernel, Point anchor, double iterations,
//							 int borderType, Scalar borderValue, Mat dst) {
//			if (kernel == null) {
//				kernel = new Mat();
//			}
//			if (anchor == null) {
//				anchor = new Point(-1, -1);
//			}
//			if (borderValue == null) {
//				borderValue = new Scalar(-1);
//			}
//			Imgproc.erode(src, dst, kernel, anchor, (int) iterations, borderType, borderValue);
//		}
//
//		/**
//		 * Segment an image based on hue, saturation, and value ranges.
//		 *
//		 * @param input The image on which to perform the HSL threshold.
//		 * @param hue   The min and max hue
//		 * @param sat   The min and max saturation
//		 * @param val   The min and max value
//		 *              //* @param output The image in which to store the output.
//		 */
//		private void hsvThreshold(Mat input, double[] hue, double[] sat, double[] val,
//								  Mat out) {
//			Imgproc.cvtColor(input, out, Imgproc.COLOR_BGR2HSV);
//			Core.inRange(out, new Scalar(hue[0], sat[0], val[0]),
//					new Scalar(hue[1], sat[1], val[1]), out);
//		}
//
//		/**
//		 * Detects groups of pixels in an image.
//		 *
//		 * @param input       The image on which to perform the find blobs.
//		 * @param minArea     The minimum size of a blob that will be found
//		 * @param circularity The minimum and maximum circularity of blobs that will be found
//		 * @param darkBlobs   The boolean that determines if light or dark blobs are found.
//		 * @param blobList    The output where the MatOfKeyPoint is stored.
//		 */
//		private void findBlobs(Mat input, double minArea, double[] circularity,
//							   Boolean darkBlobs, MatOfKeyPoint blobList) {
//			//FeatureDetector blobDet = FeatureDetector.create(FeatureDetector.SIMPLEBLOB);
//			try {
//				File tempFile = File.createTempFile("config", ".xml");
//
//				StringBuilder config = new StringBuilder();
//
//				config.append("<?xml version=\"1.0\"?>\n");
//				config.append("<opencv_storage>\n");
//				config.append("<thresholdStep>10.</thresholdStep>\n");
//				config.append("<minThreshold>50.</minThreshold>\n");
//				config.append("<maxThreshold>220.</maxThreshold>\n");
//				config.append("<minRepeatability>2</minRepeatability>\n");
//				config.append("<minDistBetweenBlobs>10.</minDistBetweenBlobs>\n");
//				config.append("<filterByColor>1</filterByColor>\n");
//				config.append("<blobColor>");
//				config.append((darkBlobs ? 0 : 255));
//				config.append("</blobColor>\n");
//				config.append("<filterByArea>1</filterByArea>\n");
//				config.append("<minArea>");
//				config.append(minArea);
//				config.append("</minArea>\n");
//				config.append("<maxArea>");
//				config.append(Integer.MAX_VALUE);
//				config.append("</maxArea>\n");
//				config.append("<filterByCircularity>1</filterByCircularity>\n");
//				config.append("<minCircularity>");
//				config.append(circularity[0]);
//				config.append("</minCircularity>\n");
//				config.append("<maxCircularity>");
//				config.append(circularity[1]);
//				config.append("</maxCircularity>\n");
//				config.append("<filterByInertia>1</filterByInertia>\n");
//				config.append("<minInertiaRatio>0.1</minInertiaRatio>\n");
//				config.append("<maxInertiaRatio>" + Integer.MAX_VALUE + "</maxInertiaRatio>\n");
//				config.append("<filterByConvexity>1</filterByConvexity>\n");
//				config.append("<minConvexity>0.95</minConvexity>\n");
//				config.append("<maxConvexity>" + Integer.MAX_VALUE + "</maxConvexity>\n");
//				config.append("</opencv_storage>\n");
//				FileWriter writer;
//				writer = new FileWriter(tempFile, false);
//				writer.write(config.toString());
//				writer.close();
//				//blobDet.read(tempFile.getPath());
//			} catch (IOException e) {
//				e.printStackTrace();
//			}
//
//			//blobDet.detect(input, blobList);
//		}
//
//
//	}
//
//}
//
